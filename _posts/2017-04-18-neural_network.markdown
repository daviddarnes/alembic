---
layout: post
title:  "神经网络"
date:   2017-04-18
author: "Joephy"
header-img: "img/post-bg-os-metro.jpg"
catalog: true
tag:
- Neural Network/神经网络
- Machine Learning/机器学习
- Introduction/介绍
comments: true
---
神经网络入门
-----------

# 第一章：概论
计算机的神经网络有两种，一种是完全模拟的，用算法来进行神经网络的模拟，另外一种是通过数字电路、模拟电路的方法把电器元件集成在一起，完成神经网络的构建和模拟。最基础的部分，是模拟神经元的结构，组成触点式的数据系统，基本上都离不开下面这条公式：

$$y_i = \sum wx-\theta$$

其中$w$是权值，$x$是上一个神经元的输出，$\theta$是阀值，$y_i$是该神经元的输出。神经元是用矩阵进行传递的，某个特定的矩阵排列代表一个特定的事物，如：[1, 1, 0]代表猫，[1, 1, 1]代表狗。因此，用数据输入的方法，然后经过神经元的计算迭代，以神经元的输出作为判断就可以做出决策。

# 第二章：前馈型神经元
### 隐含层只有1层
如果隐含层中只有一个隐单元，则：

$$h_1 = f(\sum wx-\theta)$$

这样的函数只能将一个平面分割成两块，并对平面中的点进行决策。如果隐含层中有两个隐单元，则：

$$h_1 = f(\sum w_ix_i-\theta_i)$$

$$h_2 = f(\sum w_ix_i-\theta_i)$$

这样能将一个平面分成三个或者四个部分。如果增加隐单元的数量，能将平面进行进一步的划分，函数以此类推。隐单元的种类越多，分割的方法越多。有点像线性规划，对于一个特定的问题，有最大、最小隐单元数。

### 多层Adaline算法
1. 先初始化多层中的权值（随机数作为权）；
2. 输入$x_k$，输出$y_k$，设定一个教师作为$t_k$，求出教师值和输出值之间的差；
3. 从第一层开始，找出输出值最接近0的神经元，然后改变其输出符号，改变其权值；
4. 如果改动后的输出误差变小，就接受改动，如果改变后的误差变大，就拒绝改动；
5. 转到下一个神经元，然后下一层，以此类推；
6. 继续下一个$x_k$的输入，继续训练。
可用于不变性网络的学习，就像图像识别中的旋转、平移一样。

### 非线性变化单元前馈型网络（B-P网络）
采用非线性变化的办法取代线性变换或者分段函数。

$$s_j = \sum w_i x_i - \theta_j$$

$$u_j = s_j$$

$$y_j = \frac{1}{1+e^{-u_j}}$$

学习的过程一样，但是涉及多层算法，通过逐层的修正，以达到收敛的目的。其实应该是逐层修正层与层之间的权值，然后让其与教师值比较，从而逐渐收敛到正确解。这也是一个有教师的神经网络学习方法。修正的方法采用梯度法，朝着最速下降曲线对权进行修正。但这种解法容易陷入局部最小解，并且陷入就出不来了，或者直接游荡在函数的平坦区。也有方法对其进行优化，一个是改变其迭代的步长，加快迭代；另外一个是加动量项，目的是为了记忆上一个时刻权的修改方向，为下一个时刻提供参考和一定的修正。

### B-P网络的考虑
1. 输入层和输出层的数量取决于要进行决策的数量。尽量减小系统规模，使学习时间和系统复杂性减小。
2. 对于隐含层的数量，已经证明了，对于任何闭合区间的连续函数，都可以用三层B-P网络进行逼近。

# 第三章：反馈式人工网络
对于反馈式的人工网络，其状态有几种：

1. 稳定，有可能是稳定解，可能是伪解，也有可能只是平衡点；
2. 环状，不断在一个范围内循环往复；
3. 混沌，不重复，也不停止；
4. 发散。

### 离散单层反馈（DHNN）
这种网络是一种单层的，其输入输出都为二值的反馈网络，主要用于联想记忆。当输入一个向量$I$作为一个初值时，网络通过反馈演化，从网络输出端得到一个向量$V$，而$V$是从初值演化来的一个稳定记忆。该算法除了能记住学习的东西之外，还要求能有一定的联想能力，对于一些模糊畸变的情况也要做出一定的响应，因此对权的设置有一个足够大的吸引域的要求。从此出发，用哈密顿能量函数描述输出函数的情况，并利用梯度法对权进行修改，使其能收敛。其中哈密顿函数为：

$$H =-0.5 \sum \sum J_{ij} P_i P_j - \sum H_i P_i$$

要注意的有几个：1. 注意吸引域，这与联想记忆能力有关；2. 使用正交化权；3. 确定正确的、合适的记忆容量；4. 利用哈密顿函数完成函数的构建。这个方法是由Hopfield提出的。

### 连续单层反馈
以细胞的脉冲为生物基础设计的一个神经网络，并提出实现的途径。
1. 以电位为基础，脉冲波为模型，如果一个神经细胞中的刺激积累到一定程度，就进行释放，对下一个神经元进行刺激；
2. 用电路原理形成数学模型电路；
3. 结合哈密顿函数和电路模型提出以电压电流等电路常数作为基础的能量函数；
4. 也存在一定的问题。现实中的电路规模太大，实现可能性较小，而且容易陷入局部最小值。

### 细胞神经网络（CNN）
在真实的神经系统中，神经细胞只是与周围的$10^3$的神经细胞相连，但是数学模型中，每个神经细胞之间都是有联系的，这样可能会对权值矩阵有一定的干扰。以这一思想为基础，实现一种局部联结，权可设计的人工神经网络。这种网络对二维图像的初级加工有较为好的表现，它的实现也比Hopfield的神经网络的实现要容易一点。

模型：
1. $A(i,j;k,l)$表示$C(k,l)$的输出与$C(i,j)$的连接权，$B(i,j;k,l)$表示$C(k,l)$的输入与$C(i,j)$的连接权；
2. 每个细胞的电路图一样；
3. 每个细胞反馈多少只取决于A，B，这两个矩阵也称为模版。
CNN的状态是有限的，也只是能记住有限的东西。而且也用到了能量函数，用梯度法来进行权的修改，当能量最小的时候，就是稳定点。而这时候，A是对称矩阵，表示$A(i,j;k,l) = A(k,l;i,j)$。对于某一个特定的神经元来说，它稳定的条件是$A(i,j;k,l)>C$。要注意对权进行设计，这个神经网络在对字符识别上表现比较好。

# 第四章：自组织竞争人工神经网络
对于该章节的内容，只是对基本的架构进行概括，不进行深入探究。基本的理念是一个神经细胞的兴奋会抑制其他神经细胞的表现，从而影响输出。
1. 自组织的权有两种，一种是层与层之间的权，进行信息的传递的，另外一种是层内互相抑制或者促进的权；
2. 每个神经元只对自己管辖的区域感兴趣，其他区域不参与；
3. 容易形成长时记忆和短时记忆，并且能在一定程度上相互替代（ART网络）；
4. 该网络能进行映照，也就是每个神经元对事物提取一定的特征，并且进行一定的排序，形成自抱团；
5. Fukushima设计的网络能用于人工视觉识别，可抵抗噪音和信息损失。

# 第五章：其他类型人工神经网络
### 随机神经网络
每个神经细胞神经元状态都是变化的，稳定要用概率来描述，看的是总体的状态。如果总体状态稳定了，那么就是一个稳定的解。可以用于模拟退火法，而且模拟退火法已经从理论上证明了，其一定能收敛到真实解，但是用的时间比较长。对于模拟退火法，网上也有很多相关资料，大概就是实用能量最小化的原理，慢慢降低温度，在每一步降低温度的时候，整个系统都会寻求一个最优解，然后找到最优解之后再进行下一步的计算。有点像马氏体相变中，温度慢慢降低，材料局部会形成一个能量最小的晶粒，从而达到整体的能量是最小的。

### 模糊模型
用集合的概念进行分类。对于每一个事物，比如说苹果，它既属于水果，也属于植物，所以用集合的方法用以进行决策。
如果单独用人工神经网络进行学习，每个可能性都要学习到才能找到最优解，但是精度高，需要的数据量也大。如果单独用模糊系统，可能的解有很多，但是速度快，可以用个人的经验先对其进行指导训练，但精度不可控。就像倒车那样，人工神经网络学习要覆盖整个倒车地点。如果用模糊系统，距离远一点的时候控制松一些，距离近的时候控制紧一些，便可以完成倒车，但是路径就会有很多了。所以结合这两个方法有一定的优势

### 遗传算法
用基因组筛选的方法尽心选择。可以用权作为基因组，也可以直接用解作为基因组，对基因组进行杂交、突变、筛选，以达到找到最优解的目的。

# 第六章：人工神经网络实现途径
1. 全模拟电路。用导纳作为神经元的权，MOS管作为电阻，但是不能保存权，而且精度低，接口难；
2. 全数字电路。通过逻辑门的控制，可以储存权，但是不能并行运算，较慢；
3. 混合电路，实现简单，能储存权，并且速度不慢。


参考文献：
《人工神经网络的模型及其应用》，张立明，复旦大学出版社，1993


